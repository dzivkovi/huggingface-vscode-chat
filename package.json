{
	"name": "vllm-huggingface-bridge",
	"publisher": "vllm-community",
	"displayName": "vLLM + HuggingFace Provider for GitHub Copilot",
	"description": "Access local vLLM/TGI servers and HuggingFace cloud models from GitHub Copilot Chat - perfect for air-gapped and hybrid deployments",
	"icon": "assets/vllm-hf-logo.png",
	"repository": {
		"type": "git",
		"url": "https://github.com/dzivkovi/vllm-huggingface-bridge"
	},
	"version": "1.0.0",
	"engines": {
		"vscode": "^1.104.0"
	},
	"categories": [
		"AI",
		"Chat"
	],
	"galleryBanner": {
		"color": "#100f11",
		"theme": "dark"
	},
	"badges": [],
	"keywords": ["vllm", "huggingface", "ai", "copilot", "local", "on-premise", "hybrid"],
	"homepage": "https://github.com/dzivkovi/vllm-huggingface-bridge",
	"bugs": {
		"url": "https://github.com/dzivkovi/vllm-huggingface-bridge/issues"
	},
	"license": "MIT",
	"contributes": {
		"languageModelChatProviders": [
			{
				"vendor": "vllm-bridge",
				"displayName": "vLLM + HuggingFace",
				"managementCommand": "huggingface.manage"
			}
		],
		"commands": [
			{
				"command": "huggingface.manage",
				"title": "Manage Hugging Face Provider"
			}
		],
		"configuration": {
			"title": "vLLM Bridge Settings",
			"properties": {
				"huggingface.localEndpoint": {
					"type": "string",
					"default": "http://hlo-codesentinel.wv.mentorg.com:8443",
					"description": "Local inference server endpoint (vLLM, TGI, TensorRT-LLM, etc.). Default: http://hlo-codesentinel.wv.mentorg.com:8443. Clear to disable local models."
				},
				"huggingface.tokenAllocation.inputRatio": {
					"type": "number",
					"default": 0.65,
					"minimum": 0.1,
					"maximum": 0.9,
					"description": "Ratio of context allocated for input tokens (0.1-0.9). Default: 0.65"
				},
				"huggingface.tokenAllocation.outputRatio": {
					"type": "number",
					"default": 0.15,
					"minimum": 0.05,
					"maximum": 0.5,
					"description": "Ratio of context allocated for output tokens (0.05-0.5). Default: 0.15"
				},
				"huggingface.tokenAllocation.minimumOutput": {
					"type": "number",
					"default": 100,
					"minimum": 50,
					"maximum": 1000,
					"description": "Minimum tokens always reserved for output (50-1000). Default: 100"
				},
				"huggingface.timeouts.localHealthCheck": {
					"type": "number",
					"default": 5000,
					"minimum": 1000,
					"maximum": 30000,
					"description": "Timeout for local server health checks in milliseconds (1000-30000). Default: 5000"
				},
				"huggingface.timeouts.localModelFetch": {
					"type": "number",
					"default": 5000,
					"minimum": 1000,
					"maximum": 30000,
					"description": "Timeout for fetching models from local server in milliseconds (1000-30000). Default: 5000"
				}
			}
		}
	},
	"main": "./out/extension.js",
	"scripts": {
		"vscode:prepublish": "npm run compile",
		"download-api": "dts dev",
		"postdownload-api": "dts main",
		"postinstall": "npm run download-api",
		"clean": "rm -rf out/ dist/ *.vsix .vscode-test/",
		"compile": "tsc -p ./",
		"lint": "eslint",
		"format": "prettier --write .",
		"watch": "tsc -watch -p ./",
		"test": "npm run compile && vscode-test",
		"package": "npm run compile && npx @vscode/vsce package",
		"rebuild": "npm run clean && npm run compile && npm run package",
		"release": "npm run package && mkdir -p releases && cp vllm-huggingface-bridge-*.vsix releases/ && cp releases/vllm-huggingface-bridge-$(node -p \"require('./package.json').version\").vsix releases/vllm-huggingface-bridge-latest.vsix"
	},
	"devDependencies": {
		"@eslint/js": "^9.13.0",
		"@stylistic/eslint-plugin": "^2.9.0",
		"@types/glob": "^8.1.0",
		"@types/mocha": "^10.0.6",
		"@types/node": "^22",
		"@types/vscode": "^1.103.0",
		"@vscode/dts": "^0.4.1",
		"@vscode/test-cli": "^0.0.11",
		"@vscode/test-electron": "^2.5.2",
		"eslint": "^9.13.0",
		"prettier": "^3.1.0",
		"typescript": "^5.9.2",
		"typescript-eslint": "^8.39.0"
	},
	"packageManager": "pnpm@8.15.4+sha512.0bd3a9be9eb0e9a692676deec00a303ba218ba279d99241475616b398dbaeedd11146f92c2843458f557b1d127e09d4c171e105bdcd6b61002b39685a8016b9e"
}