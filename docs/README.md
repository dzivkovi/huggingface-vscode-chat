# Documentation Index

## Essential Guides

1. **[Architecture Overview](01-architecture.md)** - System design and components
2. **[Backend Decisions](02-backend-decisions.md)** - Why vLLM over alternatives
3. **[Development Setup](03-setup-development.md)** - Build and test the extension
4. **[Local Inference Setup](04-local-inference-setup.md)** - Deploy vLLM for air-gapped environments
5. **[Installation Guide](05-installation.md)** - Install and configure the extension

## Quick Start for Common Use Cases

### Air-Gapped Deployment
Start with [Local Inference Setup](04-local-inference-setup.md)

### Cloud Development
Start with [Installation Guide](05-installation.md)

### Contributing
Start with [Development Setup](03-setup-development.md)

## Archived Documentation

Additional technical details and legacy TGI documentation available in [archive](archive/) folder:
- Detailed model benchmarks and performance metrics
- TGI setup and troubleshooting (deprecated)
- vLLM success analysis and token calculation details
- Extended troubleshooting scenarios